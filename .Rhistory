needed_any <- c("Year","Country of Origin ISO","Country of Asylum ISO","Applied")
ok <- all(needed_any %in% cols)
if(!ok){
warning("UNHCR columns not as expected. Found: ", paste(cols, collapse=", "))
}
# Detect if origin dimension missing (all '-' like current file)
if("Country of Origin ISO" %in% cols &&
length(unique(asylum_raw$`Country of Origin ISO`)) == 1 &&
unique(asylum_raw$`Country of Origin ISO`)[1] == "-"){
warning("\n*** Your current UNHCR download appears to LACK origin breakdown (Origin ISO = '-'). Re-download:\n",
"   Dataset Group: Forced displacement -> Dataset: Asylum applications\n",
"   Show BOTH Country of Origin and Country of Asylum in the table before clicking Download.\n",
"   Then rerun this script. Skipping aggregation for now. ***\n")
asylum_tidy <- tibble()
} else {
asylum_tidy <- asylum_raw %>%
filter(`Country of Origin ISO` %in% lac_iso3,
Year >= min(years), Year <= max(years)) %>%
group_by(origin_iso = `Country of Origin ISO`, year = Year) %>%
summarise(asylum_apps = sum(Applied, na.rm=TRUE), .groups="drop")
write_csv(asylum_tidy, "data/intermediate/asylum_country_year.csv")
message("UNHCR processed; rows: ", nrow(asylum_tidy))
}
}
# ------------------------------------------------------------------
# 4. UN DESA migrant stock (diaspora)
#     Your workbook has many tables; bilateral matrix is usually a separate CSV.
#     We will try to locate a sheet with 'Destination' and 'Origin' tokens.
# ------------------------------------------------------------------
undesa_path <- "data/raw/un_desa_migrant_stock.xlsx"
diaspora_interp <- tibble()
if(file.exists(undesa_path)){
sheet_names <- excel_sheets(undesa_path)
# Strategy: look for sheet whose first 100 cells contain the word "Destination"
candidate <- NULL
for(s in sheet_names){
tmp <- suppressMessages(read_excel(undesa_path, sheet = s, range = "A1:Z40"))
txt <- paste(unlist(tmp), collapse=" ")
if(str_detect(tolower(txt), "destination") & str_detect(tolower(txt), "origin")){
candidate <- s; break
}
}
if(is.null(candidate)){
warning("Could not autodetect bilateral sheet. Provide a cleaner UN DESA bilateral CSV if possible.")
} else {
message("Attempting to parse UN DESA sheet: ", candidate)
# This generic parsing may still need manual adjustment; we give a placeholder.
raw_sheet <- read_excel(undesa_path, sheet = candidate, col_names = FALSE)
# ---- QUICK OPTION (static diaspora): use 2020 total outward migrant stock to US & Spain from a simpler source later ----
# Placeholder object:
diaspora_quick <- tibble(
iso3 = lac_iso3,
year = 2020,
diaspora_US  = NA_real_,
diaspora_ESP = NA_real_
)
# Write placeholder so merging code won't fail; fill later manually or after better parsing.
write_csv(diaspora_quick, "data/intermediate/diaspora_quick_placeholder.csv")
message("Wrote diaspora_quick_placeholder.csv (fill with real stocks later).")
}
} else {
warning("UN DESA file not found. Provide diaspora data later.")
}
# ------------------------------------------------------------------
# 5. Save a minimal variable dictionary (append as you go)
# ------------------------------------------------------------------
var_dict <- tribble(
~variable, ~description, ~source,
"n_drought", "Number of drought events (EM-DAT filtered)", "EM-DAT",
"n_flood", "Number of flood events (EM-DAT filtered)", "EM-DAT",
"n_hurricane", "Number of hurricane (tropical cyclone) events", "EM-DAT",
"deaths_drought", "Total deaths from drought events", "EM-DAT",
"affected_drought", "Total people affected by drought events", "EM-DAT",
"int_disp_drought", "New internal displacements due to drought", "IDMC",
"int_disp_flood", "New internal displacements due to flood", "IDMC",
"int_disp_hurricane", "New internal displacements due to hurricane", "IDMC",
"asylum_apps", "Total asylum applications filed (all destinations)", "UNHCR",
"diaspora_US", "Migrant stock from origin living in United States (year t)", "UN DESA",
"diaspora_ESP", "Migrant stock from origin living in Spain (year t)", "UN DESA"
)
write_csv(var_dict, "docs/variable_dictionary_seed.csv")
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
## scripts/01_process_raw.R
## Reads raw datasets and produces tidy per-country-year aggregates.
pkgs <- c("readr","readxl","dplyr","stringr","tidyr","purrr","lubridate","zoo")
new <- pkgs[!pkgs %in% installed.packages()[,"Package"]]
if(length(new)) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# ------------------------------------------------------------------
# 0. Backbone (countries + years)
# ------------------------------------------------------------------
countries <- read_csv("data/intermediate/countries.csv", show_col_types = FALSE)
lac_iso3  <- countries$iso3
years     <- 2008:2023
# Helper: ensure expected range
check_years <- function(df, var="year"){
rng <- range(df[[var]], na.rm=TRUE)
message("Year coverage: ", paste(rng, collapse=" - "))
}
dir.create("data/intermediate", showWarnings = FALSE)
# ------------------------------------------------------------------
# 1. EM-DAT processing  (Excel as provided)
# ------------------------------------------------------------------
emdat_xlsx <- "data/raw/emdat_2008_2023.xlsx"
stopifnot(file.exists(emdat_xlsx))
emdat_raw <- read_excel(emdat_xlsx)
# Expected columns (sample you have)
# 'DisNo.' 'Disaster Type' 'Disaster Subtype' 'ISO' 'Country' 'Year'
# 'Total Deaths' 'No. Affected' etc.
emdat_tidy <- emdat_raw %>%
filter(ISO %in% lac_iso3,
Year >= min(years), Year <= max(years),
`Disaster Type` %in% c("Drought","Flood","Storm")) %>%
mutate(is_hurricane = str_detect(tolower(`Disaster Subtype`), "hurricane|tropical cyclone"),
hazard_group = case_when(
`Disaster Type` == "Drought" ~ "drought",
`Disaster Type` == "Flood"   ~ "flood",
`Disaster Type` == "Storm" & is_hurricane ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
# ensure numeric
mutate(
deaths   = suppressWarnings(as.numeric(`Total Deaths`)),
affected = suppressWarnings(as.numeric(`No. Affected`))
)
# Aggregate to country-year-hazard
emdat_aggr_long <- emdat_tidy %>%
group_by(ISO, Year, hazard_group) %>%
summarise(
events      = n(),
deaths      = sum(deaths, na.rm=TRUE),
affected    = sum(affected, na.rm=TRUE),
deaths_any  = as.integer(any(deaths > 0)),
affected_any= as.integer(any(affected > 0)),
.groups="drop"
)
# Wide (counts & intensity separate)
emdat_counts <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, events) %>%
pivot_wider(names_from = hazard_group, values_from = events, values_fill = 0,
names_prefix = "n_")
emdat_impact <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, deaths, affected) %>%
pivot_wider(names_from = hazard_group, values_from = c(deaths, affected),
values_fill = 0)
emdat_wide <- emdat_counts %>%
left_join(emdat_impact, by = c("ISO","Year"))
write_csv(emdat_wide, "data/intermediate/emdat_country_year.csv")
message("EM-DAT processed: ", nrow(emdat_wide), " country-year rows (some will be fewer than full panel).")
# ------------------------------------------------------------------
# 2. IDMC disaster displacement
# ------------------------------------------------------------------
idmc_csv <- "data/raw/idmc_disaster_displacements.csv"
stopifnot(file.exists(idmc_csv))
idmc_raw <- read_csv(idmc_csv, show_col_types = FALSE)
# Remove metadata row(s): they start with '#' patterns
idmc_clean <- idmc_raw %>%
filter(str_detect(iso3, "^[A-Z]{3}$"))
idmc_tidy <- idmc_clean %>%
filter(iso3 %in% lac_iso3,
year >= min(years), year <= max(years)) %>%
mutate(hazard_group = case_when(
str_detect(tolower(hazard_type_name), "drought") ~ "drought",
str_detect(tolower(hazard_type_name), "flood") ~ "flood",
str_detect(tolower(hazard_type_name), "storm|hurricane|cyclone") ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
mutate(new_disp = suppressWarnings(as.numeric(new_displacement))) %>%
group_by(iso3, year, hazard_group) %>%
summarise(new_disp = sum(new_disp, na.rm=TRUE), .groups="drop") %>%
pivot_wider(names_from = hazard_group,
values_from = new_disp,
values_fill = 0,
names_prefix = "int_disp_")
write_csv(idmc_tidy, "data/intermediate/idmc_country_year.csv")
message("IDMC processed; rows: ", nrow(idmc_tidy))
# ------------------------------------------------------------------
# 3. UNHCR asylum applications (flow)  *** VALIDATION ***
# ------------------------------------------------------------------
unhcr_csv <- "data/raw/unhcr_asylum_origin.csv"
if(!file.exists(unhcr_csv)){
warning("UNHCR file missing. Skip.")
asylum_tidy <- tibble()
} else {
asylum_raw <- read_csv(unhcr_csv, show_col_types = FALSE)
# Heuristic checks for columns
cols <- names(asylum_raw)
needed_any <- c("Year","Country of Origin ISO","Country of Asylum ISO","Applied")
ok <- all(needed_any %in% cols)
if(!ok){
warning("UNHCR columns not as expected. Found: ", paste(cols, collapse=", "))
}
# Detect if origin dimension missing (all '-' like current file)
if("Country of Origin ISO" %in% cols &&
length(unique(asylum_raw$`Country of Origin ISO`)) == 1 &&
unique(asylum_raw$`Country of Origin ISO`)[1] == "-"){
warning("\n*** Your current UNHCR download appears to LACK origin breakdown (Origin ISO = '-'). Re-download:\n",
"   Dataset Group: Forced displacement -> Dataset: Asylum applications\n",
"   Show BOTH Country of Origin and Country of Asylum in the table before clicking Download.\n",
"   Then rerun this script. Skipping aggregation for now. ***\n")
asylum_tidy <- tibble()
} else {
asylum_tidy <- asylum_raw %>%
filter(`Country of Origin ISO` %in% lac_iso3,
Year >= min(years), Year <= max(years)) %>%
group_by(origin_iso = `Country of Origin ISO`, year = Year) %>%
summarise(asylum_apps = sum(Applied, na.rm=TRUE), .groups="drop")
write_csv(asylum_tidy, "data/intermediate/asylum_country_year.csv")
message("UNHCR processed; rows: ", nrow(asylum_tidy))
}
}
# ------------------------------------------------------------------
# 4. UN DESA migrant stock (diaspora)
#     Your workbook has many tables; bilateral matrix is usually a separate CSV.
#     We will try to locate a sheet with 'Destination' and 'Origin' tokens.
# ------------------------------------------------------------------
undesa_path <- "data/raw/un_desa_migrant_stock.xlsx"
diaspora_interp <- tibble()
if(file.exists(undesa_path)){
sheet_names <- excel_sheets(undesa_path)
# Strategy: look for sheet whose first 100 cells contain the word "Destination"
candidate <- NULL
for(s in sheet_names){
tmp <- suppressMessages(read_excel(undesa_path, sheet = s, range = "A1:Z40"))
txt <- paste(unlist(tmp), collapse=" ")
if(str_detect(tolower(txt), "destination") & str_detect(tolower(txt), "origin")){
candidate <- s; break
}
}
if(is.null(candidate)){
warning("Could not autodetect bilateral sheet. Provide a cleaner UN DESA bilateral CSV if possible.")
} else {
message("Attempting to parse UN DESA sheet: ", candidate)
# This generic parsing may still need manual adjustment; we give a placeholder.
raw_sheet <- read_excel(undesa_path, sheet = candidate, col_names = FALSE)
# ---- QUICK OPTION (static diaspora): use 2020 total outward migrant stock to US & Spain from a simpler source later ----
# Placeholder object:
diaspora_quick <- tibble(
iso3 = lac_iso3,
year = 2020,
diaspora_US  = NA_real_,
diaspora_ESP = NA_real_
)
# Write placeholder so merging code won't fail; fill later manually or after better parsing.
write_csv(diaspora_quick, "data/intermediate/diaspora_quick_placeholder.csv")
message("Wrote diaspora_quick_placeholder.csv (fill with real stocks later).")
}
} else {
warning("UN DESA file not found. Provide diaspora data later.")
}
# ------------------------------------------------------------------
# 5. Save a minimal variable dictionary (append as you go)
# ------------------------------------------------------------------
var_dict <- tribble(
~variable, ~description, ~source,
"n_drought", "Number of drought events (EM-DAT filtered)", "EM-DAT",
"n_flood", "Number of flood events (EM-DAT filtered)", "EM-DAT",
"n_hurricane", "Number of hurricane (tropical cyclone) events", "EM-DAT",
"deaths_drought", "Total deaths from drought events", "EM-DAT",
"affected_drought", "Total people affected by drought events", "EM-DAT",
"int_disp_drought", "New internal displacements due to drought", "IDMC",
"int_disp_flood", "New internal displacements due to flood", "IDMC",
"int_disp_hurricane", "New internal displacements due to hurricane", "IDMC",
"asylum_apps", "Total asylum applications filed (all destinations)", "UNHCR",
"diaspora_US", "Migrant stock from origin living in United States (year t)", "UN DESA",
"diaspora_ESP", "Migrant stock from origin living in Spain (year t)", "UN DESA"
)
write_csv(var_dict, "docs/variable_dictionary_seed.csv")
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
setwd("/path/to/project")
## scripts/01_process_raw.R
## Reads raw datasets and produces tidy per-country-year aggregates.
setwd("/path/to/project")
pkgs <- c("readr","readxl","dplyr","stringr","tidyr","purrr","lubridate","zoo")
getwd()
list.files("data/raw")
file.exists("data/raw/emdat_2008_2023.xlsx")
file.exists("data/raw/idmc_disaster_displacements.csv")
file.exists("data/raw/unhcr_asylum_origin.csv")
file.exists("data/raw/un_desa_migrant_stock.xlsx")
getwd()
list.files("data/raw")
file.exists("data/raw/emdat_2008_2023.xlsx")
file.exists("data/raw/idmc_disaster_displacements.csv")
file.exists("data/raw/unhcr_asylum_origin.csv")
file.exists("data/raw/un_desa_migrant_stock.xlsx")
getwd()
list.files()
getwd()
list.files()
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(knitr)
# read the file
countries <- read_csv("data/intermediate/countries.csv", show_col_types = FALSE)
# identify the ISO column and one name column automatically
iso_col  <- intersect(names(countries), c("iso3", "iso"))[1]
name_col <- setdiff(names(countries), iso_col)[1]   # first non-ISO column
kable(
countries %>% arrange(.data[[name_col]]) %>% select(all_of(c(iso_col, name_col))),
col.names = c("ISO-3", "Country"),
caption   = "Appendix A: LAC countries included in the panel (2008–2023)"
)
knitr::opts_chunk$set(echo = TRUE)
install.packages("xfun")
install.packages("xfun")
knitr::opts_chunk$set(echo = TRUE)
install.packages("xfun")
install.packages("xfun")
options(
repos = c(CRAN = "https://cloud.r-project.org")   # sets a default mirror
)
# 1. Force-remove any loaded copy
detach("package:xfun", unload = TRUE)   # ignore warning if package not loaded
options(
repos = c(CRAN = "https://cloud.r-project.org")   # sets a default mirror
)
# 0. If you haven't already, restart R so nothing is loaded.
#    (In RStudio: Session ▶ Restart R; or quit & reopen Rgui/terminal.)
# 1. Install / upgrade xfun explicitly from a fixed mirror
install.packages("xfun", repos = "https://cloud.r-project.org")
# 2. Verify the installed version on disk
packageVersion("xfun")   # should show ‘0.48’ or higher
# 3. (Optional) make sure no old copy is loaded in this fresh session
if ("xfun" %in% loadedNamespaces()) unloadNamespace("xfun")
install.packages("xfun", repos = "https://cloud.r-project.org")
library(readr); library(dplyr); library(knitr); library(ggplot2)
panel <- read_csv("data/processed/panel_merged_wide.csv", show_col_types = FALSE)
# key vars for table
vars <- panel %>%
select(
asylum_apps, total_int_disp,
events_drought, events_flood, events_hurricane,
gdp_pc_const, agri_va_pct, unemp_rate, remit_gdp_pct
)
sumtab <- vars %>%
summarise(across(everything(),
list(Mean = ~mean(., na.rm=TRUE),
SD   = ~sd(., na.rm=TRUE),
Min  = ~min(., na.rm=TRUE),
Max  = ~max(., na.rm=TRUE)),
.names = "{.col}_{.fn}"))
# reshape for kable
table2 <- tibble(Variable = names(vars)) %>%
bind_cols(as.data.frame(t(sumtab))) %>%
select(Variable, Mean, SD, Min, Max)
library(readr); library(dplyr); library(tidyr); library(knitr); library(ggplot2)
panel <- read_csv("data/processed/panel_merged_wide.csv", show_col_types = FALSE)
# variables to display
vars <- panel %>%
select(
asylum_apps, total_int_disp,
events_drought, events_flood, events_hurricane,
gdp_pc_const, agri_va_pct, unemp_rate, remit_gdp_pct
)
# build tidy summary
sumtab <- vars %>%
summarise(across(everything(),
list(Mean = ~mean(. ,na.rm=TRUE),
SD   = ~sd(. ,na.rm=TRUE),
Min  = ~min(., na.rm=TRUE),
Max  = ~max(., na.rm=TRUE)),
.names = "{.col}_{.fn}")) %>%
pivot_longer(everything(),
names_to = c("Variable", ".value"),
names_sep = "_")
kable(sumtab, digits = 2,
caption = "Table 2: Summary statistics (2008–2023, N = 512)")
# regional flow trends plot
panel_long <- panel %>%
group_by(year) %>%
summarise(asylum = sum(asylum_apps, na.rm=TRUE),
int_disp = sum(total_int_disp, na.rm=TRUE),
.groups="drop") %>%
pivot_longer(-year, names_to = "type", values_to = "count")
ggplot(panel_long, aes(year, count/1e5, colour = type)) +
geom_line(size = 1) +
labs(y = "Count (hundreds of thousands)",
colour = "", title = "Figure 1: Regional migration flows, 2008–2023") +
theme_minimal()
library(readr); library(dplyr); library(tidyr); library(knitr); library(ggplot2)
# Use the unified panel you already built
panel <- read_csv("data/processed/panel_merged_wide.csv", show_col_types = FALSE)
# --- 5.1 Confidence intervals for mean outcomes ---
mean_ci <- function(x){
x <- x[is.finite(x)]
n  <- length(x); m <- mean(x); s <- sd(x)
se <- s / sqrt(n); tcrit <- qt(.975, df = n - 1)
tibble(N = n, Mean = m, SD = s,
`CI 95% Lower` = m - tcrit*se,
`CI 95% Upper` = m + tcrit*se)
}
ci_tbl <- bind_rows(
Asylum_Applications      = mean_ci(panel$asylum_apps),
Internal_Displacement    = mean_ci(panel$total_int_disp)
) |>
dplyr::mutate(Variable = c("Asylum applications", "Total internal displacements"),
.before = 1) |>
select(Variable, everything())
kable(ci_tbl, digits = 2,
caption = "Table: 95% confidence intervals for mean outcomes (pooled, N = 512).")
# --- 5.2 Difference-in-means tests aligned to H1 and H2 ---
panel <- panel %>%
mutate(
sudden_onset_year = as.integer(coalesce(events_flood,0) + coalesce(events_hurricane,0) > 0),
drought_year      = as.integer(coalesce(events_drought,0) > 0)
)
t_intdisp <- t.test(total_int_disp ~ sudden_onset_year, data = panel)
t_asylum  <- t.test(asylum_apps     ~ drought_year,     data = panel)
ttab <- tibble(
Test = c("Internal displacement: sudden-onset year vs none",
"Asylum applications: drought year vs none"),
`Group means` = c(
sprintf("%.0f vs %.0f",
mean(panel$total_int_disp[panel$sudden_onset_year==1], na.rm=TRUE),
mean(panel$total_int_disp[panel$sudden_onset_year==0], na.rm=TRUE)),
sprintf("%.0f vs %.0f",
mean(panel$asylum_apps[panel$drought_year==1], na.rm=TRUE),
mean(panel$asylum_apps[panel$drought_year==0], na.rm=TRUE))
),
`t-statistic` = c(unname(t_intdisp$statistic), unname(t_asylum$statistic)),
`p-value`     = c(unname(t_intdisp$p.value),   unname(t_asylum$p.value))
)
kable(ttab, digits = 3, caption = "Table: Simple t-tests (pooled) corresponding to H1 and H2.")
# --- Groupmate's original approach (uses CSVs in the working directory) ---
# Load libraries
library(tidyverse)
library(broom)
library(ggplot2)
# Load data (adjust paths if your files are in subfolders)
emdat      <- read.csv('data/raw/emdat_country_year.csv')
# --- Groupmate's original approach (uses CSVs in the working directory) ---
# Load libraries
library(tidyverse)
library(broom)
library(ggplot2)
# Load data (adjust paths if your files are in subfolders)
emdat      <- read.csv('emdat_country_year.csv')
# --- Groupmate's original approach (uses CSVs in the working directory) ---
# Load libraries
library(tidyverse)
library(broom)
library(ggplot2)
# Load data (adjust paths if your files are in subfolders)
emdat      <- read.csv('emdat_country_year.csv')
library(readr); library(dplyr); library(tidyr); library(knitr); library(ggplot2); library(broom)
# Use the unified panel you already created
# (created by scripts/02_merge_panel.R)
panel <- read_csv("data/processed/panel_merged_wide.csv", show_col_types = FALSE)
# ---------- 5.1 Confidence intervals for key outcomes ----------
mean_ci <- function(x){
x <- x[is.finite(x)]
n <- length(x); m <- mean(x); s <- sd(x)
se <- s / sqrt(n); tcrit <- qt(.975, df = n - 1)
tibble(N = n, Mean = m, SD = s,
`CI 95% Lower` = m - tcrit*se,
`CI 95% Upper` = m + tcrit*se)
}
ci_tbl <- bind_rows(
Asylum_Applications   = mean_ci(panel$asylum_apps),
Internal_Displacement = mean_ci(panel$total_int_disp)
) |>
dplyr::mutate(Variable = c("Asylum applications", "Total internal displacements"),
.before = 1) |>
select(Variable, everything())
kable(ci_tbl, digits = 2,
caption = "Table: 95% confidence intervals for mean outcomes (pooled, N = 512).")
# ---------- 5.2 Difference-in-means tests aligned to H1 and H2 ----------
panel <- panel %>%
mutate(
sudden_onset_year = as.integer(coalesce(events_flood,0) + coalesce(events_hurricane,0) > 0),
drought_year      = as.integer(coalesce(events_drought,0) > 0)
)
t_intdisp <- t.test(total_int_disp ~ sudden_onset_year, data = panel)
t_asylum  <- t.test(asylum_apps     ~ drought_year,     data = panel)
ttab <- tibble(
Test = c("Internal displacement: sudden-onset year vs none",
"Asylum applications: drought year vs none"),
`Group means` = c(
sprintf("%.0f vs %.0f",
mean(panel$total_int_disp[panel$sudden_onset_year==1], na.rm=TRUE),
mean(panel$total_int_disp[panel$sudden_onset_year==0], na.rm=TRUE)),
sprintf("%.0f vs %.0f",
mean(panel$asylum_apps[panel$drought_year==1], na.rm=TRUE),
mean(panel$asylum_apps[panel$drought_year==0], na.rm=TRUE))
),
`t-statistic` = c(unname(t_intdisp$statistic), unname(t_asylum$statistic)),
`p-value`     = c(unname(t_intdisp$p.value),   unname(t_asylum$p.value))
)
kable(ttab, digits = 3, caption = "Table: Simple t-tests (pooled) corresponding to H1 and H2.")
# ---------- 5.3 Simple OLS preview (illustrative) ----------
# (Keeps the spirit of your teammate's model but uses the correct dataset.)
ols_df <- panel %>% select(asylum_apps, events_drought, events_flood, events_hurricane, gdp_pc_const)
model <- lm(asylum_apps ~ events_drought + events_flood + events_hurricane + gdp_pc_const,
data = ols_df)
kable(tidy(model), digits = 3, caption = "OLS preview: asylum applications on disasters and GDPpc (pooled).")
# Optional: residual diagnostics plot (comment out if knitting to PDF without graphics space)
# par(mfrow = c(2,2)); plot(model)
# Save regression results if needed
# write.csv(tidy(model), "output/regression_results_ols_preview.csv", row.names = FALSE)
install.packages("xfun")
packageVersion("xfun")
warnings()
