names_prefix = "int_disp_")
write_csv(idmc_tidy, "data/intermediate/idmc_country_year.csv")
message("IDMC processed; rows: ", nrow(idmc_tidy))
# ------------------------------------------------------------------
# 3. UNHCR asylum applications (flow)  *** VALIDATION ***
# ------------------------------------------------------------------
unhcr_csv <- "data/raw/unhcr_asylum_origin.csv"
if(!file.exists(unhcr_csv)){
warning("UNHCR file missing. Skip.")
asylum_tidy <- tibble()
} else {
asylum_raw <- read_csv(unhcr_csv, show_col_types = FALSE)
# Heuristic checks for columns
cols <- names(asylum_raw)
needed_any <- c("Year","Country of Origin ISO","Country of Asylum ISO","Applied")
ok <- all(needed_any %in% cols)
if(!ok){
warning("UNHCR columns not as expected. Found: ", paste(cols, collapse=", "))
}
# Detect if origin dimension missing (all '-' like current file)
if("Country of Origin ISO" %in% cols &&
length(unique(asylum_raw$`Country of Origin ISO`)) == 1 &&
unique(asylum_raw$`Country of Origin ISO`)[1] == "-"){
warning("\n*** Your current UNHCR download appears to LACK origin breakdown (Origin ISO = '-'). Re-download:\n",
"   Dataset Group: Forced displacement -> Dataset: Asylum applications\n",
"   Show BOTH Country of Origin and Country of Asylum in the table before clicking Download.\n",
"   Then rerun this script. Skipping aggregation for now. ***\n")
asylum_tidy <- tibble()
} else {
asylum_tidy <- asylum_raw %>%
filter(`Country of Origin ISO` %in% lac_iso3,
Year >= min(years), Year <= max(years)) %>%
group_by(origin_iso = `Country of Origin ISO`, year = Year) %>%
summarise(asylum_apps = sum(Applied, na.rm=TRUE), .groups="drop")
write_csv(asylum_tidy, "data/intermediate/asylum_country_year.csv")
message("UNHCR processed; rows: ", nrow(asylum_tidy))
}
}
# ------------------------------------------------------------------
# 4. UN DESA migrant stock (diaspora)
#     Your workbook has many tables; bilateral matrix is usually a separate CSV.
#     We will try to locate a sheet with 'Destination' and 'Origin' tokens.
# ------------------------------------------------------------------
undesa_path <- "data/raw/un_desa_migrant_stock.xlsx"
diaspora_interp <- tibble()
if(file.exists(undesa_path)){
sheet_names <- excel_sheets(undesa_path)
# Strategy: look for sheet whose first 100 cells contain the word "Destination"
candidate <- NULL
for(s in sheet_names){
tmp <- suppressMessages(read_excel(undesa_path, sheet = s, range = "A1:Z40"))
txt <- paste(unlist(tmp), collapse=" ")
if(str_detect(tolower(txt), "destination") & str_detect(tolower(txt), "origin")){
candidate <- s; break
}
}
if(is.null(candidate)){
warning("Could not autodetect bilateral sheet. Provide a cleaner UN DESA bilateral CSV if possible.")
} else {
message("Attempting to parse UN DESA sheet: ", candidate)
# This generic parsing may still need manual adjustment; we give a placeholder.
raw_sheet <- read_excel(undesa_path, sheet = candidate, col_names = FALSE)
# ---- QUICK OPTION (static diaspora): use 2020 total outward migrant stock to US & Spain from a simpler source later ----
# Placeholder object:
diaspora_quick <- tibble(
iso3 = lac_iso3,
year = 2020,
diaspora_US  = NA_real_,
diaspora_ESP = NA_real_
)
# Write placeholder so merging code won't fail; fill later manually or after better parsing.
write_csv(diaspora_quick, "data/intermediate/diaspora_quick_placeholder.csv")
message("Wrote diaspora_quick_placeholder.csv (fill with real stocks later).")
}
} else {
warning("UN DESA file not found. Provide diaspora data later.")
}
# ------------------------------------------------------------------
# 5. Save a minimal variable dictionary (append as you go)
# ------------------------------------------------------------------
var_dict <- tribble(
~variable, ~description, ~source,
"n_drought", "Number of drought events (EM-DAT filtered)", "EM-DAT",
"n_flood", "Number of flood events (EM-DAT filtered)", "EM-DAT",
"n_hurricane", "Number of hurricane (tropical cyclone) events", "EM-DAT",
"deaths_drought", "Total deaths from drought events", "EM-DAT",
"affected_drought", "Total people affected by drought events", "EM-DAT",
"int_disp_drought", "New internal displacements due to drought", "IDMC",
"int_disp_flood", "New internal displacements due to flood", "IDMC",
"int_disp_hurricane", "New internal displacements due to hurricane", "IDMC",
"asylum_apps", "Total asylum applications filed (all destinations)", "UNHCR",
"diaspora_US", "Migrant stock from origin living in United States (year t)", "UN DESA",
"diaspora_ESP", "Migrant stock from origin living in Spain (year t)", "UN DESA"
)
write_csv(var_dict, "docs/variable_dictionary_seed.csv")
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
# ------------------------------------------------------------------
# 1. EM-DAT processing  (Excel as provided)
# ------------------------------------------------------------------
emdat_xlsx <- "data/raw/emdat_2008_2023.xlsx"
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
## scripts/01_process_raw.R
## Reads raw datasets and produces tidy per-country-year aggregates.
pkgs <- c("readr","readxl","dplyr","stringr","tidyr","purrr","lubridate","zoo")
new <- pkgs[!pkgs %in% installed.packages()[,"Package"]]
if(length(new)) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# ------------------------------------------------------------------
# 0. Backbone (countries + years)
# ------------------------------------------------------------------
countries <- read_csv("data/intermediate/countries.csv", show_col_types = FALSE)
lac_iso3  <- countries$iso3
years     <- 2008:2023
# Helper: ensure expected range
check_years <- function(df, var="year"){
rng <- range(df[[var]], na.rm=TRUE)
message("Year coverage: ", paste(rng, collapse=" - "))
}
dir.create("data/intermediate", showWarnings = FALSE)
# ------------------------------------------------------------------
# 1. EM-DAT processing  (Excel as provided)
# ------------------------------------------------------------------
emdat_xlsx <- "data/raw/emdat_2008_2023.xlsx"
stopifnot(file.exists(emdat_xlsx))
emdat_raw <- read_excel(emdat_xlsx)
# Expected columns (sample you have)
# 'DisNo.' 'Disaster Type' 'Disaster Subtype' 'ISO' 'Country' 'Year'
# 'Total Deaths' 'No. Affected' etc.
emdat_tidy <- emdat_raw %>%
filter(ISO %in% lac_iso3,
Year >= min(years), Year <= max(years),
`Disaster Type` %in% c("Drought","Flood","Storm")) %>%
mutate(is_hurricane = str_detect(tolower(`Disaster Subtype`), "hurricane|tropical cyclone"),
hazard_group = case_when(
`Disaster Type` == "Drought" ~ "drought",
`Disaster Type` == "Flood"   ~ "flood",
`Disaster Type` == "Storm" & is_hurricane ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
# ensure numeric
mutate(
deaths   = suppressWarnings(as.numeric(`Total Deaths`)),
affected = suppressWarnings(as.numeric(`No. Affected`))
)
# Aggregate to country-year-hazard
emdat_aggr_long <- emdat_tidy %>%
group_by(ISO, Year, hazard_group) %>%
summarise(
events      = n(),
deaths      = sum(deaths, na.rm=TRUE),
affected    = sum(affected, na.rm=TRUE),
deaths_any  = as.integer(any(deaths > 0)),
affected_any= as.integer(any(affected > 0)),
.groups="drop"
)
# Wide (counts & intensity separate)
emdat_counts <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, events) %>%
pivot_wider(names_from = hazard_group, values_from = events, values_fill = 0,
names_prefix = "n_")
emdat_impact <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, deaths, affected) %>%
pivot_wider(names_from = hazard_group, values_from = c(deaths, affected),
values_fill = 0)
emdat_wide <- emdat_counts %>%
left_join(emdat_impact, by = c("ISO","Year"))
write_csv(emdat_wide, "data/intermediate/emdat_country_year.csv")
message("EM-DAT processed: ", nrow(emdat_wide), " country-year rows (some will be fewer than full panel).")
# ------------------------------------------------------------------
# 2. IDMC disaster displacement
# ------------------------------------------------------------------
idmc_csv <- "data/raw/idmc_disaster_displacements.csv"
stopifnot(file.exists(idmc_csv))
idmc_raw <- read_csv(idmc_csv, show_col_types = FALSE)
# Remove metadata row(s): they start with '#' patterns
idmc_clean <- idmc_raw %>%
filter(str_detect(iso3, "^[A-Z]{3}$"))
idmc_tidy <- idmc_clean %>%
filter(iso3 %in% lac_iso3,
year >= min(years), year <= max(years)) %>%
mutate(hazard_group = case_when(
str_detect(tolower(hazard_type_name), "drought") ~ "drought",
str_detect(tolower(hazard_type_name), "flood") ~ "flood",
str_detect(tolower(hazard_type_name), "storm|hurricane|cyclone") ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
mutate(new_disp = suppressWarnings(as.numeric(new_displacement))) %>%
group_by(iso3, year, hazard_group) %>%
summarise(new_disp = sum(new_disp, na.rm=TRUE), .groups="drop") %>%
pivot_wider(names_from = hazard_group,
values_from = new_disp,
values_fill = 0,
names_prefix = "int_disp_")
write_csv(idmc_tidy, "data/intermediate/idmc_country_year.csv")
message("IDMC processed; rows: ", nrow(idmc_tidy))
# ------------------------------------------------------------------
# 3. UNHCR asylum applications (flow)  *** VALIDATION ***
# ------------------------------------------------------------------
unhcr_csv <- "data/raw/unhcr_asylum_origin.csv"
if(!file.exists(unhcr_csv)){
warning("UNHCR file missing. Skip.")
asylum_tidy <- tibble()
} else {
asylum_raw <- read_csv(unhcr_csv, show_col_types = FALSE)
# Heuristic checks for columns
cols <- names(asylum_raw)
needed_any <- c("Year","Country of Origin ISO","Country of Asylum ISO","Applied")
ok <- all(needed_any %in% cols)
if(!ok){
warning("UNHCR columns not as expected. Found: ", paste(cols, collapse=", "))
}
# Detect if origin dimension missing (all '-' like current file)
if("Country of Origin ISO" %in% cols &&
length(unique(asylum_raw$`Country of Origin ISO`)) == 1 &&
unique(asylum_raw$`Country of Origin ISO`)[1] == "-"){
warning("\n*** Your current UNHCR download appears to LACK origin breakdown (Origin ISO = '-'). Re-download:\n",
"   Dataset Group: Forced displacement -> Dataset: Asylum applications\n",
"   Show BOTH Country of Origin and Country of Asylum in the table before clicking Download.\n",
"   Then rerun this script. Skipping aggregation for now. ***\n")
asylum_tidy <- tibble()
} else {
asylum_tidy <- asylum_raw %>%
filter(`Country of Origin ISO` %in% lac_iso3,
Year >= min(years), Year <= max(years)) %>%
group_by(origin_iso = `Country of Origin ISO`, year = Year) %>%
summarise(asylum_apps = sum(Applied, na.rm=TRUE), .groups="drop")
write_csv(asylum_tidy, "data/intermediate/asylum_country_year.csv")
message("UNHCR processed; rows: ", nrow(asylum_tidy))
}
}
# ------------------------------------------------------------------
# 4. UN DESA migrant stock (diaspora)
#     Your workbook has many tables; bilateral matrix is usually a separate CSV.
#     We will try to locate a sheet with 'Destination' and 'Origin' tokens.
# ------------------------------------------------------------------
undesa_path <- "data/raw/un_desa_migrant_stock.xlsx"
diaspora_interp <- tibble()
if(file.exists(undesa_path)){
sheet_names <- excel_sheets(undesa_path)
# Strategy: look for sheet whose first 100 cells contain the word "Destination"
candidate <- NULL
for(s in sheet_names){
tmp <- suppressMessages(read_excel(undesa_path, sheet = s, range = "A1:Z40"))
txt <- paste(unlist(tmp), collapse=" ")
if(str_detect(tolower(txt), "destination") & str_detect(tolower(txt), "origin")){
candidate <- s; break
}
}
if(is.null(candidate)){
warning("Could not autodetect bilateral sheet. Provide a cleaner UN DESA bilateral CSV if possible.")
} else {
message("Attempting to parse UN DESA sheet: ", candidate)
# This generic parsing may still need manual adjustment; we give a placeholder.
raw_sheet <- read_excel(undesa_path, sheet = candidate, col_names = FALSE)
# ---- QUICK OPTION (static diaspora): use 2020 total outward migrant stock to US & Spain from a simpler source later ----
# Placeholder object:
diaspora_quick <- tibble(
iso3 = lac_iso3,
year = 2020,
diaspora_US  = NA_real_,
diaspora_ESP = NA_real_
)
# Write placeholder so merging code won't fail; fill later manually or after better parsing.
write_csv(diaspora_quick, "data/intermediate/diaspora_quick_placeholder.csv")
message("Wrote diaspora_quick_placeholder.csv (fill with real stocks later).")
}
} else {
warning("UN DESA file not found. Provide diaspora data later.")
}
# ------------------------------------------------------------------
# 5. Save a minimal variable dictionary (append as you go)
# ------------------------------------------------------------------
var_dict <- tribble(
~variable, ~description, ~source,
"n_drought", "Number of drought events (EM-DAT filtered)", "EM-DAT",
"n_flood", "Number of flood events (EM-DAT filtered)", "EM-DAT",
"n_hurricane", "Number of hurricane (tropical cyclone) events", "EM-DAT",
"deaths_drought", "Total deaths from drought events", "EM-DAT",
"affected_drought", "Total people affected by drought events", "EM-DAT",
"int_disp_drought", "New internal displacements due to drought", "IDMC",
"int_disp_flood", "New internal displacements due to flood", "IDMC",
"int_disp_hurricane", "New internal displacements due to hurricane", "IDMC",
"asylum_apps", "Total asylum applications filed (all destinations)", "UNHCR",
"diaspora_US", "Migrant stock from origin living in United States (year t)", "UN DESA",
"diaspora_ESP", "Migrant stock from origin living in Spain (year t)", "UN DESA"
)
write_csv(var_dict, "docs/variable_dictionary_seed.csv")
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
## scripts/01_process_raw.R
## Reads raw datasets and produces tidy per-country-year aggregates.
pkgs <- c("readr","readxl","dplyr","stringr","tidyr","purrr","lubridate","zoo")
new <- pkgs[!pkgs %in% installed.packages()[,"Package"]]
if(length(new)) install.packages(new)
invisible(lapply(pkgs, library, character.only = TRUE))
# ------------------------------------------------------------------
# 0. Backbone (countries + years)
# ------------------------------------------------------------------
countries <- read_csv("data/intermediate/countries.csv", show_col_types = FALSE)
lac_iso3  <- countries$iso3
years     <- 2008:2023
# Helper: ensure expected range
check_years <- function(df, var="year"){
rng <- range(df[[var]], na.rm=TRUE)
message("Year coverage: ", paste(rng, collapse=" - "))
}
dir.create("data/intermediate", showWarnings = FALSE)
# ------------------------------------------------------------------
# 1. EM-DAT processing  (Excel as provided)
# ------------------------------------------------------------------
emdat_xlsx <- "data/raw/emdat_2008_2023.xlsx"
stopifnot(file.exists(emdat_xlsx))
emdat_raw <- read_excel(emdat_xlsx)
# Expected columns (sample you have)
# 'DisNo.' 'Disaster Type' 'Disaster Subtype' 'ISO' 'Country' 'Year'
# 'Total Deaths' 'No. Affected' etc.
emdat_tidy <- emdat_raw %>%
filter(ISO %in% lac_iso3,
Year >= min(years), Year <= max(years),
`Disaster Type` %in% c("Drought","Flood","Storm")) %>%
mutate(is_hurricane = str_detect(tolower(`Disaster Subtype`), "hurricane|tropical cyclone"),
hazard_group = case_when(
`Disaster Type` == "Drought" ~ "drought",
`Disaster Type` == "Flood"   ~ "flood",
`Disaster Type` == "Storm" & is_hurricane ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
# ensure numeric
mutate(
deaths   = suppressWarnings(as.numeric(`Total Deaths`)),
affected = suppressWarnings(as.numeric(`No. Affected`))
)
# Aggregate to country-year-hazard
emdat_aggr_long <- emdat_tidy %>%
group_by(ISO, Year, hazard_group) %>%
summarise(
events      = n(),
deaths      = sum(deaths, na.rm=TRUE),
affected    = sum(affected, na.rm=TRUE),
deaths_any  = as.integer(any(deaths > 0)),
affected_any= as.integer(any(affected > 0)),
.groups="drop"
)
# Wide (counts & intensity separate)
emdat_counts <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, events) %>%
pivot_wider(names_from = hazard_group, values_from = events, values_fill = 0,
names_prefix = "n_")
emdat_impact <- emdat_aggr_long %>%
select(ISO, Year, hazard_group, deaths, affected) %>%
pivot_wider(names_from = hazard_group, values_from = c(deaths, affected),
values_fill = 0)
emdat_wide <- emdat_counts %>%
left_join(emdat_impact, by = c("ISO","Year"))
write_csv(emdat_wide, "data/intermediate/emdat_country_year.csv")
message("EM-DAT processed: ", nrow(emdat_wide), " country-year rows (some will be fewer than full panel).")
# ------------------------------------------------------------------
# 2. IDMC disaster displacement
# ------------------------------------------------------------------
idmc_csv <- "data/raw/idmc_disaster_displacements.csv"
stopifnot(file.exists(idmc_csv))
idmc_raw <- read_csv(idmc_csv, show_col_types = FALSE)
# Remove metadata row(s): they start with '#' patterns
idmc_clean <- idmc_raw %>%
filter(str_detect(iso3, "^[A-Z]{3}$"))
idmc_tidy <- idmc_clean %>%
filter(iso3 %in% lac_iso3,
year >= min(years), year <= max(years)) %>%
mutate(hazard_group = case_when(
str_detect(tolower(hazard_type_name), "drought") ~ "drought",
str_detect(tolower(hazard_type_name), "flood") ~ "flood",
str_detect(tolower(hazard_type_name), "storm|hurricane|cyclone") ~ "hurricane",
TRUE ~ NA_character_
)) %>%
filter(!is.na(hazard_group)) %>%
mutate(new_disp = suppressWarnings(as.numeric(new_displacement))) %>%
group_by(iso3, year, hazard_group) %>%
summarise(new_disp = sum(new_disp, na.rm=TRUE), .groups="drop") %>%
pivot_wider(names_from = hazard_group,
values_from = new_disp,
values_fill = 0,
names_prefix = "int_disp_")
write_csv(idmc_tidy, "data/intermediate/idmc_country_year.csv")
message("IDMC processed; rows: ", nrow(idmc_tidy))
# ------------------------------------------------------------------
# 3. UNHCR asylum applications (flow)  *** VALIDATION ***
# ------------------------------------------------------------------
unhcr_csv <- "data/raw/unhcr_asylum_origin.csv"
if(!file.exists(unhcr_csv)){
warning("UNHCR file missing. Skip.")
asylum_tidy <- tibble()
} else {
asylum_raw <- read_csv(unhcr_csv, show_col_types = FALSE)
# Heuristic checks for columns
cols <- names(asylum_raw)
needed_any <- c("Year","Country of Origin ISO","Country of Asylum ISO","Applied")
ok <- all(needed_any %in% cols)
if(!ok){
warning("UNHCR columns not as expected. Found: ", paste(cols, collapse=", "))
}
# Detect if origin dimension missing (all '-' like current file)
if("Country of Origin ISO" %in% cols &&
length(unique(asylum_raw$`Country of Origin ISO`)) == 1 &&
unique(asylum_raw$`Country of Origin ISO`)[1] == "-"){
warning("\n*** Your current UNHCR download appears to LACK origin breakdown (Origin ISO = '-'). Re-download:\n",
"   Dataset Group: Forced displacement -> Dataset: Asylum applications\n",
"   Show BOTH Country of Origin and Country of Asylum in the table before clicking Download.\n",
"   Then rerun this script. Skipping aggregation for now. ***\n")
asylum_tidy <- tibble()
} else {
asylum_tidy <- asylum_raw %>%
filter(`Country of Origin ISO` %in% lac_iso3,
Year >= min(years), Year <= max(years)) %>%
group_by(origin_iso = `Country of Origin ISO`, year = Year) %>%
summarise(asylum_apps = sum(Applied, na.rm=TRUE), .groups="drop")
write_csv(asylum_tidy, "data/intermediate/asylum_country_year.csv")
message("UNHCR processed; rows: ", nrow(asylum_tidy))
}
}
# ------------------------------------------------------------------
# 4. UN DESA migrant stock (diaspora)
#     Your workbook has many tables; bilateral matrix is usually a separate CSV.
#     We will try to locate a sheet with 'Destination' and 'Origin' tokens.
# ------------------------------------------------------------------
undesa_path <- "data/raw/un_desa_migrant_stock.xlsx"
diaspora_interp <- tibble()
if(file.exists(undesa_path)){
sheet_names <- excel_sheets(undesa_path)
# Strategy: look for sheet whose first 100 cells contain the word "Destination"
candidate <- NULL
for(s in sheet_names){
tmp <- suppressMessages(read_excel(undesa_path, sheet = s, range = "A1:Z40"))
txt <- paste(unlist(tmp), collapse=" ")
if(str_detect(tolower(txt), "destination") & str_detect(tolower(txt), "origin")){
candidate <- s; break
}
}
if(is.null(candidate)){
warning("Could not autodetect bilateral sheet. Provide a cleaner UN DESA bilateral CSV if possible.")
} else {
message("Attempting to parse UN DESA sheet: ", candidate)
# This generic parsing may still need manual adjustment; we give a placeholder.
raw_sheet <- read_excel(undesa_path, sheet = candidate, col_names = FALSE)
# ---- QUICK OPTION (static diaspora): use 2020 total outward migrant stock to US & Spain from a simpler source later ----
# Placeholder object:
diaspora_quick <- tibble(
iso3 = lac_iso3,
year = 2020,
diaspora_US  = NA_real_,
diaspora_ESP = NA_real_
)
# Write placeholder so merging code won't fail; fill later manually or after better parsing.
write_csv(diaspora_quick, "data/intermediate/diaspora_quick_placeholder.csv")
message("Wrote diaspora_quick_placeholder.csv (fill with real stocks later).")
}
} else {
warning("UN DESA file not found. Provide diaspora data later.")
}
# ------------------------------------------------------------------
# 5. Save a minimal variable dictionary (append as you go)
# ------------------------------------------------------------------
var_dict <- tribble(
~variable, ~description, ~source,
"n_drought", "Number of drought events (EM-DAT filtered)", "EM-DAT",
"n_flood", "Number of flood events (EM-DAT filtered)", "EM-DAT",
"n_hurricane", "Number of hurricane (tropical cyclone) events", "EM-DAT",
"deaths_drought", "Total deaths from drought events", "EM-DAT",
"affected_drought", "Total people affected by drought events", "EM-DAT",
"int_disp_drought", "New internal displacements due to drought", "IDMC",
"int_disp_flood", "New internal displacements due to flood", "IDMC",
"int_disp_hurricane", "New internal displacements due to hurricane", "IDMC",
"asylum_apps", "Total asylum applications filed (all destinations)", "UNHCR",
"diaspora_US", "Migrant stock from origin living in United States (year t)", "UN DESA",
"diaspora_ESP", "Migrant stock from origin living in Spain (year t)", "UN DESA"
)
write_csv(var_dict, "docs/variable_dictionary_seed.csv")
message("\nProcessing script finished. Next steps:\n",
" - Re-download UNHCR with origin dimension if warned.\n",
" - Replace diaspora placeholder with parsed/interpolated series.\n",
" - Proceed to merge script once asylum & diaspora tidy files exist.\n")
setwd("/path/to/project")
## scripts/01_process_raw.R
## Reads raw datasets and produces tidy per-country-year aggregates.
setwd("/path/to/project")
pkgs <- c("readr","readxl","dplyr","stringr","tidyr","purrr","lubridate","zoo")
getwd()
list.files("data/raw")
file.exists("data/raw/emdat_2008_2023.xlsx")
file.exists("data/raw/idmc_disaster_displacements.csv")
file.exists("data/raw/unhcr_asylum_origin.csv")
file.exists("data/raw/un_desa_migrant_stock.xlsx")
getwd()
list.files("data/raw")
file.exists("data/raw/emdat_2008_2023.xlsx")
file.exists("data/raw/idmc_disaster_displacements.csv")
file.exists("data/raw/unhcr_asylum_origin.csv")
file.exists("data/raw/un_desa_migrant_stock.xlsx")
getwd()
list.files()
getwd()
list.files()
